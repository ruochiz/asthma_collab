{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call peaks with ```scPrinter```\n",
    "\n",
    "- Function to use: [scprinter.pp.call_peaks](https://ruochiz.com/scprinter_doc/reference/_autosummary/scprinter.pp.call_peaks.html)\n",
    "- Tutorial to follow: [scPrinter PBMC scATAC-seq tutorial](https://ruochiz.com/scprinter_doc/tutorials/PBMC_scATAC_tutorial.html#Now-let's-use-scPrinter-for-some-basic-exploratory-analysis-to-get-a-better-idea-of-the-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import scprinter as scp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "from scanpy.plotting.palettes import zeileis_28\n",
    "from tqdm.contrib.concurrent import *\n",
    "from tqdm.auto import *\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import statistics as stat\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snapatac2 as snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0a'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scprinter.genome.Genome at 0x7f8c8d351110>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the reference genome. This must match that of your ATAC fragments file\n",
    "genome = scp.genome.mm10\n",
    "\n",
    "genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromVAR_or_seq2PRINT = 'chromvar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data_dir = '/bap/bap/collab_asthma_multiome/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs\n",
    "printer_h5ad_output_dir = os.path.join(master_data_dir, 'ATAC', '2_Analysis_Outputs', '1b_ChromVAR_scPrinter_object')\n",
    "scprinter_obj_path = os.path.join(printer_h5ad_output_dir, 'Asthma_Multiome_Collab_scPrinter.h5ad')\n",
    "\n",
    "output_dir = os.path.join(master_data_dir, 'ATAC', '2_Analysis_Outputs', f'1c_{chromVAR_or_seq2PRINT}_scPrinter_MACS_peaks')\n",
    "\n",
    "# if the output directory does not exist, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1c_chromvar_scPrinter_MACS_peaks'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prep paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small lambda function to get the path to the data, input variable being sample name\n",
    "get_condition_fragments_path = lambda sample_name_bc, sample_name_frag: os.path.join(master_data_dir, 'ATAC', 'ATACFragmentFiles_Asthma', sample_name_bc, f'{sample_name_frag}_atac_fragments.tsv.gz')\n",
    "get_condition_valid_barcodes_path = lambda sample_name: os.path.join(master_data_dir, 'outputs', 'ATAC', '2_Analysis_Outputs', '1a_ChromVAR_Inputs', f'{sample_name}_valid_barcodes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample names\n",
    "sample_names_bc = ['NT',\n",
    "                'OVA_C',\n",
    "                'OVA',\n",
    "                'PBS_C',\n",
    "                'PBS'\n",
    "                ]\n",
    "\n",
    "# on-disk fragments files are named slightly differently\n",
    "sample_names_load_fragments = ['NT',\n",
    "                                'OVAC',\n",
    "                                'OVA',\n",
    "                                'PBSC',\n",
    "                                'PBS'\n",
    "                                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to per-condition fragments\n",
    "\n",
    "fragment_paths_l = []\n",
    "valid_barcodes_l = []   # order-matched to fragment_paths_l\n",
    "for sample_name_fragments_i, sample_name_bc_i in zip(sample_names_load_fragments, sample_names_bc):\n",
    "    fragment_paths_l.append(get_condition_fragments_path(sample_name_bc_i, sample_name_fragments_i))\n",
    "    valid_barcodes_l.append(get_condition_valid_barcodes_path(sample_name_bc_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you'll likely need txt files of barcodes:subtype pairings per condition too,\n",
    "# when you do the manual t-test later and need to group barcodes by subtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ```scPrinter``` analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the scPrinter object\n",
    "\n",
    "When you finish using the object, **run ```printer.close()``` otherwise you won't be able to load it properly next time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer = scp.load_printer(scprinter_obj_path, genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head project\n",
      "AnnData object with n_obs x n_vars = 7418 x 0 backed at '/bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter.h5ad'\n",
      "    obs: 'sample', 'n_fragment', 'frac_dup', 'frac_mito', 'frag_path', 'frag_sample_name', 'tsse'\n",
      "    uns: 'unique_string', 'genome', 'gff_db', 'insertion', 'footprints', 'bias_bw', 'bias_path', 'binding score', 'reference_sequences'\n",
      "    obsm: 'insertion_chr18', 'insertion_chr7', 'insertion_chr8', 'insertion_chrY', 'insertion_chr4', 'insertion_chr19', 'insertion_chr12', 'insertion_chr13', 'insertion_chr5', 'insertion_chr2', 'insertion_chr3', 'insertion_chr14', 'insertion_chr6', 'insertion_chr9', 'insertion_chrX', 'insertion_chr17', 'insertion_chr1', 'insertion_chr10', 'insertion_chr11', 'insertion_chr15', 'insertion_chr16'\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Peak calling (seq2PRINT preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 duplicate barcodes:\n",
      "{'AAGTAGCCATAATCCG-1', 'CCTGTTGGTTAATGAC-1', 'GCGAAGTAGGCAGGTG-1', 'CAAAGGATCATGTGGT-1', 'AGTAAGTAGTTAGAGG-1', 'TTGGGCCAGGACCTGC-1', 'ACTTGTCGTTCCGGGA-1', 'CTCTAGCTCATGCTTT-1', 'GGTAACCGTGCTGTAA-1', 'CTTACCGGTTGCAATG-1', 'AATAGCTGTCAAGTAT-1', 'TCAAGGAAGCGGTTAT-1', 'GCGCGATTCCCATAGG-1', 'AGCAACAAGCCGCAGT-1'}\n"
     ]
    }
   ],
   "source": [
    "# Check uniqueness of barcodes\n",
    "\n",
    "# Load each text file of valid barcodes into a big list and then check for duplicates\n",
    "valid_barcodes = []\n",
    "for valid_barcodes_path_i in valid_barcodes_l:\n",
    "    with open(valid_barcodes_path_i, 'r') as f:\n",
    "        valid_barcodes_i = f.readlines()\n",
    "        valid_barcodes_i = [x.strip() for x in valid_barcodes_i]\n",
    "        valid_barcodes.extend(valid_barcodes_i)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_barcodes = set([x for x in valid_barcodes if valid_barcodes.count(x) > 1])\n",
    "if len(duplicate_barcodes) > 0:\n",
    "    print(f'Found {len(duplicate_barcodes)} duplicate barcodes:')\n",
    "    print(duplicate_barcodes)\n",
    "else:\n",
    "    print('No duplicate barcodes found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NT', 'OVA_C', 'OVA', 'PBS_C', 'PBS']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_names_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bap/bap/collab_asthma_multiome/outputs/ATAC/2_Analysis_Outputs/1a_ChromVAR_Inputs/NT_valid_barcodes.txt',\n",
       " '/bap/bap/collab_asthma_multiome/outputs/ATAC/2_Analysis_Outputs/1a_ChromVAR_Inputs/OVA_C_valid_barcodes.txt',\n",
       " '/bap/bap/collab_asthma_multiome/outputs/ATAC/2_Analysis_Outputs/1a_ChromVAR_Inputs/OVA_valid_barcodes.txt',\n",
       " '/bap/bap/collab_asthma_multiome/outputs/ATAC/2_Analysis_Outputs/1a_ChromVAR_Inputs/PBS_C_valid_barcodes.txt',\n",
       " '/bap/bap/collab_asthma_multiome/outputs/ATAC/2_Analysis_Outputs/1a_ChromVAR_Inputs/PBS_valid_barcodes.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_barcodes_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in valid_barcodes_l as a nested list\n",
    "\n",
    "valid_bc = list(printer.obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7418"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do ChromVAR analysis here, so let's comment out what we would do if we were preparing peaks for seq2PRINT (note: we wouldn't lump all cells together for that anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here we call peaks on all pass-QC cells across all subtypes and all conditions/samples, for ChromVAR analysis\n",
    "\n",
    "# scp.pp.call_peaks(printer=printer,\n",
    "#                   frag_file=fragment_paths_l,\n",
    "#                   cell_grouping=[valid_bc], \n",
    "#                   group_names=['all'],\n",
    "#                   sample_names=sample_names_bc,\n",
    "#                   preset='seq2PRINT',\n",
    "#                   overwrite=True,\n",
    "#                   merge_across_groups=True,\n",
    "#                   n_jobs=1\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printer.uns[\"peak_calling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetched the cleaned peaks, save, it will be used in the next step\n",
    "# cleaned_peaks_seq2PRINT = pd.DataFrame(printer.uns[\"peak_calling\"]['merged'][:])\n",
    "# cleaned_peaks_seq2PRINT.to_csv(f'{output_dir}/seq2PRINT_preset_Asthma_Multiome_scPrinter_cleaned_narrowPeak.bed',\n",
    "#                      sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Peak calling (ChromVAR preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running macs2 with macs2 callpeak --nomodel -t /bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter_supp/chromvar_all_filtered_frag.tsv.gz --outdir /bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter_supp/macs2 -n chromvar_all -f BEDPE --nolambda --keep-dup all --call-summits --nomodel -B --SPMR --shift 75 --extsize 150 -q 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ Thu, 27 Feb 2025 14:21:05: \n",
      "# Command line: callpeak --nomodel -t /bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter_supp/chromvar_all_filtered_frag.tsv.gz --outdir /bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter_supp/macs2 -n chromvar_all -f BEDPE --nolambda --keep-dup all --call-summits --nomodel -B --SPMR --shift 75 --extsize 150 -q 0.01\n",
      "# ARGUMENTS LIST:\n",
      "# name = chromvar_all\n",
      "# format = BEDPE\n",
      "# ChIP-seq file = ['/bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter_supp/chromvar_all_filtered_frag.tsv.gz']\n",
      "# control file = None\n",
      "# effective genome size = 2.70e+09\n",
      "# band width = 300\n",
      "# model fold = [5, 50]\n",
      "# qvalue cutoff = 1.00e-02\n",
      "# The maximum gap between significant sites is assigned as the read length/tag size.\n",
      "# The minimum length of peaks is assigned as the predicted fragment length \"d\".\n",
      "# Larger dataset will be scaled towards smaller dataset.\n",
      "# Range for calculating regional lambda is: 10000 bps\n",
      "# Broad region calling is off\n",
      "# Paired-End mode is on\n",
      "# Searching for subpeak summits is on\n",
      "# MACS will save fragment pileup signal per million reads\n",
      " \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:05: #1 read fragment files... \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:05: #1 read treatment fragments... \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:06:  1000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:07:  2000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:08:  3000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:09:  4000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:10:  5000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:11:  6000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:12:  7000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:13:  8000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:14:  9000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:15:  10000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:16:  11000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:17:  12000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:18:  13000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:19:  14000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:20:  15000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:21:  16000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:22:  17000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:23:  18000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:24:  19000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:25:  20000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:26:  21000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:27:  22000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:28:  23000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:29:  24000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:30:  25000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:31:  26000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:32:  27000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:33:  28000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:34:  29000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:35:  30000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:36:  31000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:37:  32000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:38:  33000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:39:  34000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:40:  35000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:41:  36000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:42:  37000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:43:  38000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:44:  39000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:45:  40000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:46:  41000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:47:  42000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:48:  43000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:49:  44000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:50:  45000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:51:  46000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:52:  47000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:53:  48000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:54:  49000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:55:  50000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:56:  51000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:57:  52000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:58:  53000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:21:59:  54000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:00:  55000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:01:  56000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:02:  57000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:03:  58000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:04:  59000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:05:  60000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:06:  61000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:07:  62000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:08:  63000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:09:  64000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:10:  65000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:11:  66000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:12:  67000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:13:  68000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:14:  69000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:15:  70000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:16:  71000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:17:  72000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:18:  73000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:19:  74000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:20:  75000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:21:  76000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:22:  77000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:23:  78000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:24:  79000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:25:  80000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:26:  81000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:26:  82000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:27:  83000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:28:  84000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:29:  85000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:30:  86000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:31:  87000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:32:  88000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:33:  89000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:34:  90000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:35:  91000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:36:  92000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:37:  93000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:38:  94000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:39:  95000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:40:  96000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:41:  97000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:42:  98000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:43:  99000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:44:  100000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:45:  101000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:46:  102000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:47:  103000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:48:  104000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:49:  105000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:50:  106000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:51:  107000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:52:  108000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:53:  109000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:54:  110000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:55:  111000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:56:  112000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:57:  113000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:58:  114000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:22:59:  115000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:00:  116000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:01:  117000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:02:  118000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:03:  119000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:04:  120000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:05:  121000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:06:  122000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:07:  123000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:08:  124000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:09:  125000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:10:  126000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:11:  127000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:12:  128000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:13:  129000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:14:  130000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:15:  131000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:16:  132000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:17:  133000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:18:  134000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:19:  135000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:20:  136000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:21:  137000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:22:  138000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:23:  139000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:24:  140000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:25:  141000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:26:  142000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:27:  143000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:28:  144000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:29:  145000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:30:  146000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:31:  147000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:32:  148000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:33:  149000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:34:  150000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:35:  151000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:36:  152000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:37:  153000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:38:  154000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:39:  155000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:40:  156000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:41:  157000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:42:  158000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:43:  159000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:44:  160000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:45:  161000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:46:  162000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:47:  163000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:48:  164000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:49:  165000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:50:  166000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:51:  167000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:52:  168000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:53:  169000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:54:  170000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:55:  171000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:56:  172000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:57:  173000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:58:  174000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:23:59:  175000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:00:  176000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:01:  177000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:02:  178000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:03:  179000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:04:  180000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:05:  181000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:06:  182000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:07:  183000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:08:  184000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:09:  185000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:10:  186000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:11:  187000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:12:  188000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:13:  189000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:14:  190000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:15:  191000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:16:  192000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:17:  193000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:18:  194000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:19:  195000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:20:  196000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:21:  197000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:22:  198000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:23:  199000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:24:  200000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:25:  201000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:26:  202000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:27:  203000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:28:  204000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:29:  205000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:30:  206000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:31:  207000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:32:  208000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:33:  209000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:34:  210000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:35:  211000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:36:  212000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:37:  213000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:38:  214000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:39:  215000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:40:  216000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:41:  217000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:42:  218000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:43:  219000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:44:  220000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:45:  221000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:46:  222000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:47:  223000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:48:  224000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:49:  225000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:50:  226000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:51:  227000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:52:  228000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:53:  229000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:54:  230000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:55:  231000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:56:  232000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:57:  233000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:58:  234000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:24:59:  235000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:00:  236000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:01:  237000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:02:  238000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:03:  239000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:04:  240000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:05:  241000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:06:  242000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:06:  243000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:07:  244000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:08:  245000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:09:  246000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:10:  247000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:11:  248000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:12:  249000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:13:  250000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:14:  251000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:15:  252000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:16:  253000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:17:  254000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:18:  255000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:19:  256000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:20:  257000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:21:  258000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:22:  259000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:23:  260000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:24:  261000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:25:  262000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:26:  263000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:27:  264000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:28:  265000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:29:  266000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:30:  267000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:31:  268000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:32:  269000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:33:  270000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:34:  271000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:35:  272000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:36:  273000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:37:  274000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:38:  275000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:39:  276000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:40:  277000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:41:  278000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:42:  279000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:43:  280000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:44:  281000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:45:  282000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:46:  283000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:47:  284000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:48:  285000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:49:  286000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:50:  287000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:51:  288000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:51:  289000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:52:  290000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:53:  291000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:54:  292000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:55:  293000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:56:  294000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:57:  295000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:58:  296000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:25:59:  297000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:00:  298000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:01:  299000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:02:  300000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:03:  301000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:04:  302000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:05:  303000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:06:  304000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:07:  305000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:08:  306000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:09:  307000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:10:  308000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:11:  309000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:12:  310000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:13:  311000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:14:  312000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:15:  313000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:16:  314000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:17:  315000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:18:  316000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:19:  317000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:20:  318000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:21:  319000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:22:  320000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:23:  321000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:24:  322000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:25:  323000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:26:  324000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:27:  325000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:28:  326000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:29:  327000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:30:  328000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:31:  329000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:32:  330000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:33:  331000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:34:  332000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:35:  333000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:36:  334000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:37:  335000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:38:  336000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:39:  337000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:40:  338000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:41:  339000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:42:  340000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:43:  341000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:44:  342000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:45:  343000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:46:  344000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:47:  345000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:48:  346000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:49:  347000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:50:  348000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:51:  349000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:52:  350000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:53:  351000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:54:  352000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:55:  353000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:56:  354000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:57:  355000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:58:  356000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:26:59:  357000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:27:00:  358000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:27:01:  359000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:27:02:  360000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:27:02:  361000000 \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #1 mean fragment size is determined as 177.6 bp from treatment \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #1 fragment size = 177.6 \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #1  total fragments in treatment: 361224640 \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #1 finished! \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #2 Build Peak Model... \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #2 Skipped... \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #3 Call peaks... \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: # local lambda is disabled! \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #3 !!!! DYNAMIC LAMBDA IS DISABLED !!!! \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #3 Going to call summits inside each peak ... \n",
      "INFO  @ Thu, 27 Feb 2025 14:31:16: #3 Pre-compute pvalue-qvalue table... \n",
      "INFO  @ Thu, 27 Feb 2025 14:32:41: #3 In the peak calling step, the following will be performed simultaneously: \n",
      "INFO  @ Thu, 27 Feb 2025 14:32:41: #3   Write bedGraph files for treatment pileup (after scaling if necessary)... chromvar_all_treat_pileup.bdg \n",
      "INFO  @ Thu, 27 Feb 2025 14:32:41: #3   Write bedGraph files for control lambda (after scaling if necessary)... chromvar_all_control_lambda.bdg \n",
      "INFO  @ Thu, 27 Feb 2025 14:32:41: #3   --SPMR is requested, so pileup will be normalized by sequencing depth in million reads. \n",
      "INFO  @ Thu, 27 Feb 2025 14:32:41: #3 Call peaks for each chromosome... \n",
      "INFO  @ Thu, 27 Feb 2025 14:37:00: #4 Write output xls file... /bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter_supp/macs2/chromvar_all_peaks.xls \n",
      "INFO  @ Thu, 27 Feb 2025 14:37:02: #4 Write peak in narrowPeak format file... /bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter_supp/macs2/chromvar_all_peaks.narrowPeak \n",
      "INFO  @ Thu, 27 Feb 2025 14:37:03: #4 Write summits bed file... /bap/bap/collab_asthma_multiome/ATAC/2_Analysis_Outputs/1b_ChromVAR_scPrinter_object/Asthma_Multiome_Collab_scPrinter_supp/macs2/chromvar_all_summits.bed \n",
      "INFO  @ Thu, 27 Feb 2025 14:37:03: Done! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in peak summit file(s):\n",
      "NOTE: Assuming all start coordinates are 0-based ..\n",
      "\n",
      "Padding peak summits by: 400 bp on either side for\n",
      "Removing peaks overlapping with blacklisted regions and out of bound peaks based on chromosome sizes ..\n",
      "\n",
      "Filtering overlapping peaks based on peak summit score ..\n",
      "round: 1 576819 peaks unresolved 324636 peaks selected\n",
      "round: 2 83434 peaks unresolved 37626 peaks selected\n",
      "round: 3 16765 peaks unresolved 6631 peaks selected\n",
      "round: 4 3974 peaks unresolved 1475 peaks selected\n",
      "round: 5 998 peaks unresolved 355 peaks selected\n",
      "round: 6 260 peaks unresolved 98 peaks selected\n",
      "round: 7 54 peaks unresolved 22 peaks selected\n",
      "round: 8 10 peaks unresolved 3 peaks selected\n",
      "round: 9 4 peaks unresolved 1 peaks selected\n",
      "round: 10 1 peaks unresolved 1 peaks selected\n",
      "finish clearing\n",
      "finish sorting\n",
      "finished summary\n"
     ]
    }
   ],
   "source": [
    "# Call peaks using chromvar preset, this set of peak are recommended to be use as cell x peak for scATAC-seq data, or analysis\n",
    "scp.pp.call_peaks(printer=printer,\n",
    "                  frag_file=fragment_paths_l,\n",
    "                  cell_grouping=[valid_bc], \n",
    "                  group_names=['chromvar_all'],\n",
    "                  sample_names=sample_names_bc,\n",
    "                  preset='chromvar',\n",
    "                  overwrite=True,\n",
    "                  merge_across_groups=True,\n",
    "                  n_jobs=1\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1627622/1189696960.py:1: DeprecationWarning: `Series._import_from_c` is deprecated. use _import_arrow_from_c; if you are using an extension, please compile it with latest 'pyo3-polars'\n",
      "  printer.uns[\"peak_calling\"].keys()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['chromvar_all', 'merged'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printer.uns[\"peak_calling\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1627622/52693986.py:1: DeprecationWarning: `Series._import_from_c` is deprecated. use _import_arrow_from_c; if you are using an extension, please compile it with latest 'pyo3-polars'\n",
      "  pd.DataFrame(printer.uns[\"peak_calling\"]['merged'][:]).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(370848, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(printer.uns[\"peak_calling\"]['merged'][:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1627622/1842597009.py:1: DeprecationWarning: `Series._import_from_c` is deprecated. use _import_arrow_from_c; if you are using an extension, please compile it with latest 'pyo3-polars'\n",
      "  pd.DataFrame(printer.uns[\"peak_calling\"]['chromvar_all'][:]).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(583083, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(printer.uns[\"peak_calling\"]['chromvar_all'][:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1627622/2289798895.py:2: DeprecationWarning: `Series._import_from_c` is deprecated. use _import_arrow_from_c; if you are using an extension, please compile it with latest 'pyo3-polars'\n",
      "  cleaned_peaks_chromVAR = pd.DataFrame(printer.uns[\"peak_calling\"]['merged'][:])\n"
     ]
    }
   ],
   "source": [
    "# Fetched the cleaned peaks, save, it will be used in the next step\n",
    "cleaned_peaks_chromVAR = pd.DataFrame(printer.uns[\"peak_calling\"]['merged'][:])\n",
    "cleaned_peaks_chromVAR.to_csv(f'{output_dir}/chromVAR_preset_Asthma_Multiome_scPrinter_cleaned_merged_narrowPeak.bed',\n",
    "                     sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Close ```printer``` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scprinter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
